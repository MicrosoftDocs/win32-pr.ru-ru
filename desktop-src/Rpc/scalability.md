---
title: Масштабируемость
description: Масштабируемость
ms.assetid: 39327621-b536-4494-9319-9e9d4f534123
keywords:
- Масштабируемость
- Удаленный вызов процедур RPC, рекомендации, масштабируемость
ms.topic: article
ms.date: 05/31/2018
ms.openlocfilehash: 0728e35d9c9b27494014363c448be9965e39eea7
ms.sourcegitcommit: 2d531328b6ed82d4ad971a45a5131b430c5866f7
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 09/16/2019
ms.locfileid: "103888972"
---
# <a name="scalability"></a><span data-ttu-id="c01ed-105">Масштабируемость</span><span class="sxs-lookup"><span data-stu-id="c01ed-105">Scalability</span></span>

<span data-ttu-id="c01ed-106">Термин «масштабируемость» зачастую используется нечасто.</span><span class="sxs-lookup"><span data-stu-id="c01ed-106">The term, scalability, is often misused.</span></span> <span data-ttu-id="c01ed-107">Для этого раздела предлагается двойное определение:</span><span class="sxs-lookup"><span data-stu-id="c01ed-107">For this section, a dual definition is provided:</span></span>

-   <span data-ttu-id="c01ed-108">Масштабируемость — это возможность полностью использовать доступную вычислительную мощность в многопроцессорной системе (2, 4, 8, 32 или больше процессоров).</span><span class="sxs-lookup"><span data-stu-id="c01ed-108">Scalability is the ability to fully utilize available processing power on a multiprocessor system (2, 4, 8, 32, or more processors).</span></span>
-   <span data-ttu-id="c01ed-109">Масштабируемость — это возможность обслуживания большого количества клиентов.</span><span class="sxs-lookup"><span data-stu-id="c01ed-109">Scalability is the ability to service a large number of clients.</span></span>

<span data-ttu-id="c01ed-110">Эти два связанных определения обычно называют *масштабированием*.</span><span class="sxs-lookup"><span data-stu-id="c01ed-110">These two related definitions are commonly referred to as *scaling up*.</span></span> <span data-ttu-id="c01ed-111">В конце этого раздела приводятся советы по *масштабированию*.</span><span class="sxs-lookup"><span data-stu-id="c01ed-111">The end of this topic provides tips about *scaling out*.</span></span>

<span data-ttu-id="c01ed-112">В этом обсуждении основное внимание уделяется исключительно написанию масштабируемых серверов, а не масштабируемых клиентов, поскольку масштабируемые серверы являются более распространенными требованиями.</span><span class="sxs-lookup"><span data-stu-id="c01ed-112">This discussion focuses exclusively on writing scalable servers, not scalable clients, because scalable servers are more common requirements.</span></span> <span data-ttu-id="c01ed-113">В этом разделе также рассматривается масштабируемость только в контексте серверов RPC и RPC.</span><span class="sxs-lookup"><span data-stu-id="c01ed-113">This section also addresses scalability in the context of RPC and RPC servers only.</span></span> <span data-ttu-id="c01ed-114">Рекомендации по масштабируемости, такие как уменьшение количества конфликтов, предотвращение частого попадания в кэш в глобальных областях памяти или предотвращение ложного совместного доступа, не рассматриваются здесь.</span><span class="sxs-lookup"><span data-stu-id="c01ed-114">Best practices for scalability, such as reducing contention, avoiding frequent cache misses on global memory locations, or avoiding false sharing, are not discussed here.</span></span>

## <a name="rpc-threading-model"></a><span data-ttu-id="c01ed-115">Потоковая модель RPC</span><span class="sxs-lookup"><span data-stu-id="c01ed-115">RPC Threading Model</span></span>

<span data-ttu-id="c01ed-116">Когда сервер получает вызов RPC, вызывается Серверная подпрограммы (подпрограммы диспетчера) для потока, предоставляемого RPC.</span><span class="sxs-lookup"><span data-stu-id="c01ed-116">When an RPC call is received by a server, the server routine (manager routine) is called on a thread supplied by RPC.</span></span> <span data-ttu-id="c01ed-117">RPC использует адаптивный пул потоков, который увеличивается и уменьшается по мере колебаний рабочей нагрузки.</span><span class="sxs-lookup"><span data-stu-id="c01ed-117">RPC uses an adaptive thread pool that increases and decreases as workload fluctuates.</span></span> <span data-ttu-id="c01ed-118">Начиная с Windows 2000, ядро пула потоков RPC является портом завершения.</span><span class="sxs-lookup"><span data-stu-id="c01ed-118">Starting with Windows 2000, the core of the RPC thread pool is a completion port.</span></span> <span data-ttu-id="c01ed-119">Порт завершения и его использование RPC настроены для выполнения действий, отводимых от 0 до небольших конфликтов.</span><span class="sxs-lookup"><span data-stu-id="c01ed-119">The completion port and its usage by RPC are tuned for zero to low contention server routines.</span></span> <span data-ttu-id="c01ed-120">Это означает, что пул потоков RPC агрессивно увеличивает число потоков обслуживания, если некоторые из них блокируются.</span><span class="sxs-lookup"><span data-stu-id="c01ed-120">This means that the RPC thread pool aggressively increases the number of servicing threads if some become blocked.</span></span> <span data-ttu-id="c01ed-121">Он работает с предположения, который блокируется редко, и если поток блокируется, это временное условие, которое быстро разрешается.</span><span class="sxs-lookup"><span data-stu-id="c01ed-121">It operates on the presumption that blocking is rare, and if a thread gets blocked, this is a temporary condition that is quickly resolved.</span></span> <span data-ttu-id="c01ed-122">Такой подход обеспечивает эффективность для небольших серверов состязаний.</span><span class="sxs-lookup"><span data-stu-id="c01ed-122">This approach enables efficiency for low contention servers.</span></span> <span data-ttu-id="c01ed-123">Например, вызов RPC-сервера, работающего на 8-процессорном 550MHz сервере, доступ к которому осуществляется через высокоскоростную системную сеть (SAN), обеспечивает более 30 000 вызовов в секунду от более 200 удаленных клиентов.</span><span class="sxs-lookup"><span data-stu-id="c01ed-123">For example, a void call RPC server operating on an eight-processor 550MHz server accessed over a high speed system area network (SAN) serves over 30,000 void calls per second from over 200 remote clients.</span></span> <span data-ttu-id="c01ed-124">Он представляет более 108 000 000 вызовов в час.</span><span class="sxs-lookup"><span data-stu-id="c01ed-124">This represents more than 108 million calls per hour.</span></span>

<span data-ttu-id="c01ed-125">В результате интенсивный пул потоков фактически получается в том случае, если состязание на сервере велико.</span><span class="sxs-lookup"><span data-stu-id="c01ed-125">The result is that the aggressive thread pool actually gets in the way when contention on the server is high.</span></span> <span data-ttu-id="c01ed-126">Для иллюстрации представьте себе, что сервер высокой нагрузки используется для удаленного доступа к файлам.</span><span class="sxs-lookup"><span data-stu-id="c01ed-126">To illustrate, imagine a heavy-duty server used to remotely access files.</span></span> <span data-ttu-id="c01ed-127">Предположим, что сервер использует наиболее простой подход: он просто считывает и записывает файл синхронно в потоке, в котором RPC вызывает серверную подпрограммы.</span><span class="sxs-lookup"><span data-stu-id="c01ed-127">Assume the server adopts the most straightforward approach: it simply reads/writes the file synchronously on the thread on which that RPC invokes the server routine.</span></span> <span data-ttu-id="c01ed-128">Кроме того, предположим, что у нас есть 4-процессорный сервер, обслуживающий множество клиентов.</span><span class="sxs-lookup"><span data-stu-id="c01ed-128">Also, assume we have a four-processor server serving many clients.</span></span>

<span data-ttu-id="c01ed-129">Сервер будет начинаться с пяти потоков (в действительности это меняется, но для простоты используются пять потоков).</span><span class="sxs-lookup"><span data-stu-id="c01ed-129">The server will start with five threads (this actually varies, but five threads is used for simplicity).</span></span> <span data-ttu-id="c01ed-130">После того как RPC выберет первый вызов RPC, он отправляет вызов серверной процедуры, а Серверная подпрограммы выдает операции ввода-вывода.</span><span class="sxs-lookup"><span data-stu-id="c01ed-130">Once RPC picks up the first RPC call, it dispatches the call to the server routine, and the server routine issues the I/O.</span></span> <span data-ttu-id="c01ed-131">Нередко она пропустила файловый кэш, а затем блокирует ожидание результата.</span><span class="sxs-lookup"><span data-stu-id="c01ed-131">Infrequently, it misses the file cache and then blocks waiting for the result.</span></span> <span data-ttu-id="c01ed-132">Как только он блокируется, пятый поток освобождается для получения запроса, а шестой поток создается как горячая резервная.</span><span class="sxs-lookup"><span data-stu-id="c01ed-132">As soon as it blocks, the fifth thread is released to pick up a request, and a sixth thread is created as a hot standby.</span></span> <span data-ttu-id="c01ed-133">Предполагая, что каждая десятая операция ввода-вывода пропустила кэш и будет блокироваться в течение 100 миллисекунд (произвольное значение времени) и при условии, что 4-процессорный сервер обслуживает около 20 000 вызовов в секунду (5 000 вызовов на процессор), упрощенное моделирование порождает, что каждый процессор будет порождать примерно 50 потоков.</span><span class="sxs-lookup"><span data-stu-id="c01ed-133">Assuming each tenth I/O operation misses the cache and will block for 100 milliseconds (an arbitrary time value), and assuming the four-processor server serves about 20,000 calls per second (5,000 calls per processor), a simplistic modeling would predict that each processor will spawn approximately 50 threads.</span></span> <span data-ttu-id="c01ed-134">Это предполагает, что вызов, который будет блокироваться, происходит каждые 2 миллисекунд, и после 100 миллисекунд первый поток освобождается снова, поэтому пул будет стабилизироваться около 200 потоков (50 на каждый процессор).</span><span class="sxs-lookup"><span data-stu-id="c01ed-134">This assumes a call that will block comes every 2 milliseconds, and after 100 milliseconds the first thread is freed again so the pool will stabilize at about 200 threads (50 per processor).</span></span>

<span data-ttu-id="c01ed-135">Фактическое поведение более сложно, так как большое число потоков приведет к появлению дополнительных переключений контекста, которые замедляют работу сервера, а также снижают скорость создания новых потоков, но основная идея очевидна.</span><span class="sxs-lookup"><span data-stu-id="c01ed-135">The actual behavior is more complicated, as the high number of threads will cause extra context switches which slow the server, and also slow the rate of creation of new threads, but the basic idea is clear.</span></span> <span data-ttu-id="c01ed-136">Число потоков быстро начинает выполняться по мере того, как потоки на сервере начинают блокироваться и ожидают чего-либо (это операция ввода-вывода или доступа к ресурсу).</span><span class="sxs-lookup"><span data-stu-id="c01ed-136">The number of threads goes up quickly as threads on the server start blocking and waiting for something (be it an I/O, or access to a resource).</span></span>

<span data-ttu-id="c01ed-137">RPC и порт завершения, на которых поступают входящие запросы, будут пытаться поддерживать количество используемых на сервере потоков RPC, равное числу процессоров на компьютере.</span><span class="sxs-lookup"><span data-stu-id="c01ed-137">RPC and the completion port that gates incoming requests will try to maintain the number of usable RPC threads in the server to be equal to the number of processors on the machine.</span></span> <span data-ttu-id="c01ed-138">Это означает, что на сервере с четырьмя процессорами, когда поток возвращается в RPC, при наличии четырех или более используемых потоков RPC пятый поток не может выбрать новый запрос, а вместо этого будет находиться в состоянии горячей замены в случае, если один из используемых в настоящий момент блоков потоков.</span><span class="sxs-lookup"><span data-stu-id="c01ed-138">This means that on a four-processor server, once a thread returns to RPC, if there are four or more usable RPC threads, the fifth thread is not allowed to pick up a new request, and instead will sit in a hot standby state in case one of the currently usable threads blocks.</span></span> <span data-ttu-id="c01ed-139">Если Пятый поток ожидает достаточно длительного времени в режиме "горячего" резервирования без количества используемых потоков RPC, которые ниже количества процессоров, он будет освобожден, то есть пул потоков будет уменьшаться.</span><span class="sxs-lookup"><span data-stu-id="c01ed-139">If the fifth thread waits long enough as a hot standby without the number of usable RPC threads dropping below the number of processors, it will be released, that is, the thread pool will decrease.</span></span>

<span data-ttu-id="c01ed-140">Представьте себе сервер с большим количеством потоков.</span><span class="sxs-lookup"><span data-stu-id="c01ed-140">Imagine a server with many threads.</span></span> <span data-ttu-id="c01ed-141">Как было сказано выше, сервер RPC завершает работу с большим количеством потоков, но только в случае частого блокирования потоков.</span><span class="sxs-lookup"><span data-stu-id="c01ed-141">As previously explained, an RPC server ends up with many threads, but only if the threads block often.</span></span> <span data-ttu-id="c01ed-142">На сервере, на котором потоки часто блокируются, поток, возвращающий RPC, вскоре выводится из списка "горячего" резервирования, так как все используемые в настоящий момент потоки блокируются и получают запрос на обработку.</span><span class="sxs-lookup"><span data-stu-id="c01ed-142">On a server where threads often block, a thread that returns to RPC is soon taken out of the hot standby list, because all currently usable threads block, and is given a request to process.</span></span> <span data-ttu-id="c01ed-143">Когда поток блокируется диспетчером потоков в контексте коммутаторов ядра в другом потоке.</span><span class="sxs-lookup"><span data-stu-id="c01ed-143">When a thread blocks, the thread dispatcher in the kernel switches context to another thread.</span></span> <span data-ttu-id="c01ed-144">Этот контекстный переключатель сам по себе потребляет циклы ЦП.</span><span class="sxs-lookup"><span data-stu-id="c01ed-144">This context switch by itself consumes CPU cycles.</span></span> <span data-ttu-id="c01ed-145">Следующий поток будет выполнять другой код, обращаться к различным структурам данных и будет иметь другой стек. Это означает, что частота попаданий в кэш памяти (кэш L1 и L2) будет значительно ниже, что приведет к более медленному выполнению.</span><span class="sxs-lookup"><span data-stu-id="c01ed-145">The next thread will be executing different code, accessing different data structures, and will have a different stack, which means the memory cache hit rate (the L1 and L2 caches) will be much lower, resulting in slower execution.</span></span> <span data-ttu-id="c01ed-146">Многочисленные одновременно выполняющиеся потоки увеличивают состязание за существующие ресурсы, такие как куча, критические разделы в серверном коде и т. д.</span><span class="sxs-lookup"><span data-stu-id="c01ed-146">The numerous threads executing simultaneously increases contention for existing resources, such as heap, critical sections in the server code, and so on.</span></span> <span data-ttu-id="c01ed-147">Это еще больше повышает состязание, как колонны в формах ресурсов.</span><span class="sxs-lookup"><span data-stu-id="c01ed-147">This further increases contention as convoys on resources form.</span></span> <span data-ttu-id="c01ed-148">Если память мала, то недостаток памяти, вызванный большим и увеличивающимся числом потоков, вызовет сбои страниц, что еще больше увеличивает скорость блокировки потоков и вызывает создание еще большего числа потоков.</span><span class="sxs-lookup"><span data-stu-id="c01ed-148">If memory is low, the memory pressure exerted by the large and growing number of threads will cause page faults, which further increase the rate at which the threads block, and cause even more threads to be created.</span></span> <span data-ttu-id="c01ed-149">В зависимости от частоты блокировки и объема доступной физической памяти сервер может либо стабилизироваться на более низком уровне производительности с частотой переключения контекста, либо снижаться до того момента, когда к жесткому диску и переключению контекста происходит только многократный доступ без выполнения реальной работы.</span><span class="sxs-lookup"><span data-stu-id="c01ed-149">Depending on how often it blocks and how much physical memory is available, the server may either stabilize at some lower level of performance with a high context switch rate, or it may deteriorate to the point where it is only repeatedly accessing the hard disk and context switching without performing any actual work.</span></span> <span data-ttu-id="c01ed-150">В этой ситуации не будет отображаться небольшая рабочая нагрузка, разумеется, но большая рабочая нагрузка быстро выводит проблему на поверхность.</span><span class="sxs-lookup"><span data-stu-id="c01ed-150">This situation will not show under light workload, of course, but a heavy workload quickly brings the problem to the surface.</span></span>

<span data-ttu-id="c01ed-151">Как это можно предотвратить?</span><span class="sxs-lookup"><span data-stu-id="c01ed-151">How can this be prevented?</span></span> <span data-ttu-id="c01ed-152">Если предполагается, что потоки блокируются, объявите вызовы как асинхронные и после того, как запрос введет серверную подсистему, помещает его в пул рабочих потоков, использующих асинхронные возможности системы ввода-вывода и (или) RPC.</span><span class="sxs-lookup"><span data-stu-id="c01ed-152">If threads are expected to block, declare calls as asynchronous, and once the request enters the server routine, queue it to a pool of worker threads that use the asynchronous capabilities of the I/O system and/or RPC.</span></span> <span data-ttu-id="c01ed-153">Если сервер, в свою очередь, выполняет вызовы RPC, сделайте их асинхронными и убедитесь, что очередь не слишком велика.</span><span class="sxs-lookup"><span data-stu-id="c01ed-153">If the server is in turn making RPC calls make those asynchronous, and make sure the queue does not grow too large.</span></span> <span data-ttu-id="c01ed-154">Если серверная часть выполняет файловый ввод-вывод, используйте асинхронный файловый ввод-вывод для постановки в очередь нескольких запросов к системе ввода-вывода и постановки в очередь всего нескольких потоков и получения результатов.</span><span class="sxs-lookup"><span data-stu-id="c01ed-154">If the server routine is performing file I/O, use asynchronous file I/O to queue multiple requests to the I/O system and have only a few threads queue them and pick up the results.</span></span> <span data-ttu-id="c01ed-155">Если серверная процедура выполняет операции сетевого ввода-вывода, снова используйте асинхронные возможности системы, чтобы выдать запросы и получать ответы асинхронно и использовать как можно меньше потоков.</span><span class="sxs-lookup"><span data-stu-id="c01ed-155">If the server routine is doing network I/O, again, use the asynchronous capabilities of the system to issue the requests and pick up the replies asynchronously, and use as few threads as possible.</span></span> <span data-ttu-id="c01ed-156">По завершении ввода-вывода или вызова RPC, выполненного сервером, завершите асинхронный вызов RPC, который доставил запрос.</span><span class="sxs-lookup"><span data-stu-id="c01ed-156">When the I/O is done, or the RPC call the server made is complete, complete the asynchronous RPC call that delivered the request.</span></span> <span data-ttu-id="c01ed-157">Это позволит серверу работать с минимально возможным количеством потоков, что повышает производительность и число клиентов, которые сервер может обслуживать.</span><span class="sxs-lookup"><span data-stu-id="c01ed-157">This will enable the server to run with as few threads as possible, which increases the performance and the number of clients a server can service.</span></span>

## <a name="scale-out"></a><span data-ttu-id="c01ed-158">Горизонтальное увеличение масштаба</span><span class="sxs-lookup"><span data-stu-id="c01ed-158">Scale Out</span></span>

<span data-ttu-id="c01ed-159">RPC может быть настроен для работы с балансировкой сетевой нагрузки (NLB), если NLB настроена так, что все запросы от заданного адреса клиента отправляются на один и тот же сервер.</span><span class="sxs-lookup"><span data-stu-id="c01ed-159">RPC can be configured to work with Network Load Balancing (NLB) if NLB is configured such that all requests from a given client address go to the same server.</span></span> <span data-ttu-id="c01ed-160">Поскольку каждый клиент RPC открывает пул подключений (Дополнительные сведения см. в разделе [RPC и сеть](rpc-and-the-network.md)), важно, чтобы все соединения из пула данного клиента настроились на одном и том же компьютере сервера.</span><span class="sxs-lookup"><span data-stu-id="c01ed-160">Because each RPC client opens a connection pool (for more information, see [RPC and the Network](rpc-and-the-network.md)), it is essential that all connections from the pool of the given client end up on the same server computer.</span></span> <span data-ttu-id="c01ed-161">При условии соблюдения этого условия можно настроить кластер балансировки сетевой нагрузки для работы в качестве одного большого сервера RPC с потенциально отличной масштабируемостью.</span><span class="sxs-lookup"><span data-stu-id="c01ed-161">As long as this condition is met, an NLB cluster can be configured to function as one large RPC server with potentially excellent scalability.</span></span>

 

 




